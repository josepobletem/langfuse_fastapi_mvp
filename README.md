
# Langfuse FastAPI Mejorado

Aplicaci√≥n de ejemplo en **FastAPI** con integraci√≥n opcional de **Langfuse**, monitoreo con **Prometheus**, logging estructurado y guardrails b√°sicos para interacciones seguras con modelos de lenguaje (LLM).

Este proyecto muestra c√≥mo construir un **servicio de IA robusto y listo para producci√≥n** con:
- ‚úÖ Observabilidad (m√©tricas, logs, trazas)
- ‚úÖ Guardrails (truncado de palabras, heur√≠stica b√°sica de toxicidad)
- ‚úÖ Resiliencia (reintentos con backoff exponencial)
- ‚úÖ CI/CD con linting, tipado, tests, Docker y validaci√≥n de Terraform
- ‚úÖ Dise√±o preparado para la nube (app 12-factor con configuraci√≥n en `.env`)

## üîç Langfuse (observabilidad de LLMs)

Langfuse est√° pensado espec√≠ficamente para aplicaciones que llaman a modelos de lenguaje (LLMs).

En tu proyecto se usa para:

Trazas (trace) ‚Üí cada request de usuario se guarda como una traza en Langfuse.
Ejemplo: cuando alguien llama a /ask, se registra user_id, la pregunta y metadatos como versi√≥n del prompt.

Generaciones (generation) ‚Üí cada llamada al modelo (ej. OpenAI GPT) se registra como una "generaci√≥n".
Incluye el input, el output, el modelo usado y las m√©tricas de tokens.

Scores (score) ‚Üí puedes a√±adir m√©tricas personalizadas para evaluar la calidad de las respuestas.
En el ejemplo:

non_empty_answer ‚Üí mide si el modelo devolvi√≥ texto vac√≠o o no.

toxicity_safe ‚Üí heur√≠stica que marca si el texto es seguro (no t√≥xico).

üëâ Resumen: Langfuse = observabilidad enfocada en el comportamiento y calidad de las respuestas del LLM.

Sirve para debugging, mejorar prompts y evaluar la performance de tu app de IA.

## üìä Prometheus (monitoreo gen√©rico de aplicaci√≥n)

Prometheus se usa para m√©tricas de infraestructura y performance de la API, no del modelo en s√≠.

En tu proyecto:

Contadores

app_requests_total{endpoint,method,status} ‚Üí n√∫mero de requests recibidas por endpoint/m√©todo/status.

Latencia

app_request_latency_seconds{endpoint,method} ‚Üí histogramas de tiempo de respuesta por endpoint.

app_llm_latency_seconds{model} ‚Üí latencia de las llamadas al modelo LLM.

Gauge

app_requests_in_progress ‚Üí cu√°ntas requests est√°n en curso en este momento.

Custom

app_llm_tokens_used ‚Üí tokens consumidos por el modelo.

üëâ Resumen: Prometheus = monitoreo cl√°sico de servicios, pensado para alertas e infraestructura.
Sirve para saber si tu API est√° r√°pida, cu√°ntos requests recibe, cu√°ntos errores hay, etc.

## üß© C√≥mo se complementan

Prometheus te da la visi√≥n de la salud y rendimiento del servicio (requests, latencia, errores).

Langfuse te da la visi√≥n de la calidad y trazabilidad de las respuestas del modelo (qu√© pregunt√≥ el usuario, qu√© respondi√≥ el LLM, si fue seguro/√∫til).

En conjunto:

Si ves que /ask tarda 3 segundos en Prometheus, en Langfuse puedes inspeccionar la traza y ver qu√© modelo respondi√≥ lento.

Si un usuario se queja de una respuesta ofensiva, en Langfuse puedes revisar la generaci√≥n, y en Prometheus ver cu√°ntas veces ocurri√≥.


---

## üìã Caracter√≠sticas

- **Endpoints**
  - `GET /health` ‚Üí Estado de la aplicaci√≥n y configuraci√≥n
  - `GET /metrics` ‚Üí M√©tricas Prometheus (latencia, requests, tokens, etc.)
  - `POST /ask` ‚Üí Endpoint de Preguntas y Respuestas con OpenAI (mockeado en tests)

- **Monitoreo**
  - M√©tricas de Prometheus: contadores de requests, histogramas de latencia, tokens usados, latencia de LLM
  - Logs en JSON con `request_id` para correlaci√≥n
  - Trazas y m√©tricas opcionales en Langfuse

- **Guardrails**
  - Truncado por n√∫mero de palabras (`max_words`) con puntos suspensivos
  - Heur√≠stica de toxicidad b√°sica
  - Scores en Langfuse: `non_empty_answer` y `toxicity_safe`

- **CI/CD**
  - Linting: `black`, `isort`, `flake8`
  - Tipado: `mypy`
  - Tests: `pytest`, `pytest-cov`
  - Seguridad: `bandit`, `pip-audit`
  - Tests de integraci√≥n con FastAPI real + `curl`
  - Validaci√≥n de build Docker
  - Validaci√≥n de Terraform (`fmt`, `validate`, `plan`) sin cuenta en la nube

---

## ‚öôÔ∏è Instalaci√≥n y Ejecuci√≥n Local

```bash
# Clonar el repo
git clone https://github.com/<tu-usuario>/langfuse-fastapi-enhanced.git
cd langfuse-fastapi-enhanced

# Crear entorno virtual
python -m venv .venv
source .venv/bin/activate      # Windows: .\.venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt

# Levantar servidor
uvicorn src.app:app --host 0.0.0.0 --port 8000 --reload
```

Visitar:  
- Health check ‚Üí [http://localhost:8000/health](http://localhost:8000/health)  
- M√©tricas ‚Üí [http://localhost:8000/metrics](http://localhost:8000/metrics)  

---

## ‚úÖ Ejemplo de Request

```bash
curl -X POST http://localhost:8000/ask   -H "Content-Type: application/json"   -d '{
    "user_id": "jose",
    "question": "¬øQu√© es Langfuse?",
    "max_words": 20
  }'
```

Respuesta:
```json
{
  "answer": "Langfuse es una plataforma de observabilidad para aplicaciones LLM‚Ä¶",
  "trace_id": "abc123",
  "generation_id": "gen456",
  "request_id": "uuid-789"
}
```

---

## üìä Observabilidad

- **Integraci√≥n Prometheus**  
  Ejemplo de scraping:
  ```yaml
  scrape_configs:
    - job_name: "fastapi-app"
      metrics_path: /metrics
      static_configs:
        - targets: ["localhost:8000"]
  ```

- **M√©tricas exportadas**
  - `app_requests_total{endpoint,method,status}`
  - `app_request_latency_seconds{endpoint,method}`
  - `app_llm_latency_seconds{model}`
  - `app_requests_in_progress`
  - `app_llm_tokens_used`

- **Logs estructurados en JSON**
  ```json
  {"level":"INFO","name":"app","msg":"request completed in 0.123s","request_id":"uuid"}
  ```

---

## üîí Guardrails

- **Truncado**: Respuestas limitadas por `max_words`.
- **Filtro de toxicidad**: Detecta palabras prohibidas.
- **Scores Langfuse**: `non_empty_answer`, `toxicity_safe`.

---

## üåê Configuraci√≥n

Todas las variables se cargan desde `.env`:

`.env.example`
```ini
OPENAI_API_KEY=sk-...
LANGFUSE_PUBLIC_KEY=pk-...
LANGFUSE_SECRET_KEY=sk-...
LANGFUSE_HOST=https://cloud.langfuse.com

APP_ENV=dev
LOG_LEVEL=INFO
PORT=8000
```

Si no defines claves, la app corre en modo degradado (sin LLM ni Langfuse).

---

## üß™ Tests

```bash
pytest -q
pytest --cov=src --cov-report=term-missing
```

---

## üê≥ Docker

```bash
docker build -t langfuse-fastapi-demo:latest .
docker run -p 8000:8000 --env-file .env langfuse-fastapi-demo:latest
```

---

## ‚ö° CI/CD Workflows

- `.github/workflows/ci.yml` ‚Üí Linting, tipado, tests, integraci√≥n, seguridad, Docker.
- `.github/workflows/release-ghcr.yml` ‚Üí Publica im√°genes Docker en GHCR con tags.
- `.github/workflows/terraform.yml` ‚Üí Validaci√≥n de Terraform offline.

---

## üì¶ Terraform (local)

Stack m√≠nimo usando solo proveedores locales (`random`, `null`, `local`):

```bash
cd infra/terraform
terraform init -backend=false
terraform validate
terraform plan
terraform apply
```

Genera un archivo local como demo.

---

## ‚ú® Cr√©ditos

Desarrollado con ‚ù§Ô∏è por **Jos√© Poblete M.**  
*"Construyendo sistemas de IA robustos, observables y listos para producci√≥n."*
